# Copyright 2025 ZTE Corporation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import inspect
import json
import sys
import time
import hashlib
from typing import List, Dict, Any

import asyncio
from concurrent.futures import ThreadPoolExecutor
from app.agent_dispatcher.domain.plan.action.skill.mcp.engine import MCPEngine
from app.agent_dispatcher.infrastructure.entity.AgentInstance import AgentInstance
from app.cosight.agent.base.skill_to_tool import convert_skill_to_tool,get_mcp_tools,convert_mcp_tools
from app.cosight.llm.chat_llm import ChatLLM
from app.cosight.task.time_record_util import time_record
from app.cosight.tool.tool_result_processor import ToolResultProcessor
from app.cosight.task.plan_report_manager import plan_report_event_manager
from app.common.logger_util import logger
from app.cosight.agent.base.tool_arg_mapping import FUNCTION_ARG_MAPPING


class BaseAgent:
    def __init__(self, agent_instance: AgentInstance, llm: ChatLLM, functions: {}, plan_id: str = None):
        self.agent_instance = agent_instance
        self.llm = llm
        self.tools = []
        self.mcp_tools = []
        self.mcp_tools = get_mcp_tools(self.agent_instance.template.skills)
        for skill in self.agent_instance.template.skills:
            self.tools.extend(convert_skill_to_tool(skill.model_dump(), 'en'))
        self.tools.extend(convert_mcp_tools(self.mcp_tools))
        self.functions = functions
        self.history = []
        self.plan_id = plan_id
        self._tool_event_sequence = 0  # Tool event sequence number
        self._file_saver_call_count = {}  # Record file_saver call count for each step
        
        # Tool call optimization
        self._tool_cache = {}  # Cache tool results
        self._batch_tools = True  # Enable batching
        self._cache_ttl = 300  # Cache TTL in seconds (5 minutes)
        self._cache_timestamps = {}  # Track cache timestamps
        
        # Only set plan to None if it hasn't been set by subclass
        if not hasattr(self, 'plan'):
            self.plan = None  # Will be set by subclasses that have access to Plan

    def _normalize_tool_args(self, function_to_call, raw_args: Dict[str, Any], function_name: str = "") -> Dict[str, Any]:
        """
        Normalize potentially non-standard parameter keys generated by LLM to real parameter names of tool functions.

        Rules:
        - Based on target function signature parameter set, only fill parameters that exist in the signature
        - Use general alias table for matching (e.g., file->filename, filepath->filename, text->content, etc.)
        - Support underscore-free/case-insensitive matching
        - Keep keys not in signature as-is (so functions can receive **kwargs)
        """
        try:
            signature = inspect.signature(function_to_call)
            param_names = set(signature.parameters.keys())

            def normalize_key(k: str) -> str:
                return (k or '').replace('_', '').lower()

            # Function name level mapping only: alias(lower)->canonical
            alias_reverse = {}
            fn_key = (function_name or '').lower()
            mapping_cfg = FUNCTION_ARG_MAPPING.get(fn_key, {})
            aliases_cfg = mapping_cfg.get('aliases', {})
            for canonical, aliases in aliases_cfg.items():
                alias_reverse[normalize_key(canonical)] = canonical
                for a in aliases:
                    alias_reverse[normalize_key(a)] = canonical

            normalized_args: Dict[str, Any] = dict(raw_args) if isinstance(raw_args, dict) else {}

            # Map alias keys to canonical keys in signature; preserve unmapped keys
            produced: Dict[str, Any] = {}
            used_keys = set()

            for key, val in list(normalized_args.items()):
                key_norm = normalize_key(key)

                # If original key is already in signature, use it directly
                if key in param_names:
                    produced[key] = val
                    used_keys.add(key)
                    continue

                # Try to reverse lookup canonical using alias
                if key_norm in alias_reverse:
                    canonical = alias_reverse[key_norm]
                    if canonical in param_names and canonical not in produced:
                        produced[canonical] = val
                        used_keys.add(key)
                        logger.info(f"Tool args normalized: {key} -> {canonical}")
                        continue

                # Try fuzzy: match signature parameters without underscores
                for p in param_names:
                    if normalize_key(p) == key_norm and p not in produced:
                        produced[p] = val
                        used_keys.add(key)
                        logger.info(f"Tool args normalized (fuzzy): {key} -> {p}")
                        break

            # Add back unused original keys (may be used for **kwargs)
            for key, val in normalized_args.items():
                if key not in used_keys and key not in produced:
                    produced[key] = val

            # Required field validation (if required is configured)
            required = mapping_cfg.get('required', [])
            missing = [r for r in required if r in param_names and r not in produced]
            if missing:
                logger.warning(f"Missing required args for {function_name}: {missing}")

            return produced
        except Exception as e:
            logger.warning(f"args normalization failed: {e}")
            return raw_args

    def find_mcp_tool(self, tool_name):
        for tool in self.mcp_tools:
            for func in tool['mcp_tools']:
                if func.name == tool_name:
                    return tool, func.name
        return None

    def _push_tool_event(self, event_type: str, tool_name: str, tool_args: str = "", 
                        tool_result: str = "", step_index: int = None, duration: float = None, 
                        error: str = None):
        """
        Push tool execution event to queue
        
        Args:
            event_type: Event type ('tool_start', 'tool_complete', 'tool_error')
            tool_name: Tool name
            tool_args: Tool arguments
            tool_result: Tool result
            step_index: Step index
            duration: Execution duration (seconds)
            error: Error message
        """
        try:
            # Relax early exit condition: report as long as there's plan_id; don't publish if no plan_id
            if not getattr(self, 'plan_id', None):
                return
            
            # Filter MCP tool events: don't send MCP tool events with step_index=-1 to frontend
            if step_index == -1:
                logger.debug(f"Skip sending MCP tool event to frontend: {event_type} for {tool_name}")
                return
            
            # Add sequence number to ensure event order
            self._tool_event_sequence += 1
            
            # Build event data
            event_data = {
                "event_type": event_type,
                "tool_name": tool_name,
                "tool_name_zh": self._get_tool_name_zh(tool_name),
                "tool_args": tool_args,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "step_index": step_index,
                "sequence": self._tool_event_sequence  # Add sequence number
            }
            
            if duration is not None:
                event_data["duration"] = round(duration, 2)
            
            if error:
                event_data["error"] = error
            elif tool_result:
                # Process tool result
                task_title = self.plan.title if self.plan else ""
                processed_result = ToolResultProcessor.process_tool_result(tool_name, tool_args, tool_result, task_title)
                event_data["processed_result"] = processed_result
                event_data["raw_result_length"] = len(tool_result)
                
                # Inject verification info, including URL
                self._inject_verification_info(event_data, tool_name, processed_result)
            
            # Publish event with routing key (plan_id)
            plan_report_event_manager.publish("tool_event", self.plan_id, event_data)
            logger.info(f"Pushed tool event: {event_type} for {tool_name}")
            
        except Exception as e:
            logger.error(f"Failed to push tool event: {e}")

    def _inject_verification_info(self, event_data: dict, tool_name: str, processed_result: dict):
        """Inject verification info into event data"""
        try:
            # Get verification steps
            steps = self._get_verification_steps(tool_name)
            
            # Extract URLs and file paths from processed_result
            urls = []
            file_path = None
            
            if isinstance(processed_result, dict):
                # Get from urls field
                if "urls" in processed_result and isinstance(processed_result["urls"], list):
                    urls.extend(processed_result["urls"])
                # Get from first_url field
                if "first_url" in processed_result and processed_result["first_url"]:
                    urls.append(processed_result["first_url"])
                
                # Get file path from file_path field
                if "file_path" in processed_result and processed_result["file_path"]:
                    file_path = processed_result["file_path"]
            
            # Ensure extra field exists
            if "extra" not in event_data:
                event_data["extra"] = {}
            
            # Build verification info
            verification_info = {
                "steps": steps,
                "urls": urls
            }
            
            # Add file path to verification info if available
            if file_path:
                verification_info["file_path"] = file_path
            
            # Inject verification info
            event_data["extra"]["verification"] = verification_info
            
        except Exception as e:
            logger.error(f"Failed to inject verification info: {e}")

    def _get_tool_name_zh(self, tool_name: str) -> str:
        """Get Chinese translation of tool name"""
        tool_name_mapping = {
            # Search tools
            "search_baidu": "百度搜索",
            "search_google": "谷歌搜索", 
            "tavily_search": "Tavily搜索",
            "search_wiki": "维基百科搜索",
            "image_search": "图片搜索",
            "audio_recognition": "音频识别",
            
            # File operation tools
            "file_saver": "保存文件",
            "file_read": "读取文件",
            "file_write": "写入文件",
            "file_append": "追加文件",
            "file_delete": "删除文件",
            "file_list": "列出文件",
            "file_copy": "复制文件",
            "file_move": "移动文件",
            "file_str_replace": "文件内容替换",
            "file_find_in_content": "文件内容查找",
            
            # Code execution tools
            # "code_executor": "代码执行器",
            "execute_code": "代码执行器",
            "python_executor": "Python执行器",
            "shell_executor": "Shell执行器",
            
            # Web operation tools
            "web_scraper": "网页抓取",
            "web_navigator": "网页导航",
            "web_click": "网页点击",
            "web_input": "网页输入",
            "web_screenshot": "网页截图",
            "browser_use": "浏览器操作",
            "fetch_website_content": "抓取网页内容",
            
            # Image analysis tools
            "image_analyzer": "图像分析",
            "image_ocr": "图像识别",
            "image_caption": "图像描述",
            
            # Video analysis tools
            "video_analyzer": "视频分析",
            "video_extract": "视频提取",
            
            # Document processing tools
            "create_html_report": "创建HTML报告",
            "document_processor": "文档处理",
            "pdf_reader": "PDF阅读器",
            "word_processor": "Word处理",
            "excel_processor": "Excel处理",
            "extract_document_content": "抽取文档内容",
            
            # Database tools
            "database_query": "数据库查询",
            "sql_executor": "SQL执行器",
            
            # Network tools
            "http_request": "HTTP请求",
            "api_call": "API调用",
            "webhook": "Webhook",
            
            # Plan management tools
            "create_plan": "创建计划",
            "update_plan": "更新计划",
            "mark_step": "标记步骤",
            "terminate": "终止任务",
            
            # Other tools
            "calculator": "计算器",
            "translator": "翻译器",
            "summarizer": "摘要器",
            "text_analyzer": "文本分析",
            "data_processor": "数据处理",
            "chart_generator": "图表生成",
            "report_generator": "报告生成"
        }
        
        return tool_name_mapping.get(tool_name, tool_name)

    def reset_step_file_saver_count(self, step_index: int):
        """Reset file_saver call count for specified step"""
        if step_index in self._file_saver_call_count:
            del self._file_saver_call_count[step_index]
            logger.info(f"Reset file_saver call count for step {step_index}")

    def _get_verification_steps(self, tool_name: str) -> list[str]:
        """Get verification steps based on tool name"""
        name = (tool_name or "").lower()
        
        # Search tools: support cross-validation
        if name in ("search_baidu", "search_google", "tavily_search", "search_wiki", "image_search"):
            return ["source_trace", "rule_assist", "self_consistency"]
        
        # Save tools
        if name in ("file_saver",):
            return ["source_trace"]
        
        # File processing tools
        if name in ("file_read", "file_find_in_content", "file_str_replace"):
            return ["rule_assist"]
        
        # Code execution/data processing
        if name in ("execute_code",):
            return ["rule_assist", "self_consistency"]
        
        # Browser scraping
        if name in ("browser_use", "fetch_website_content"):
            return ["source_trace", "rule_assist"]
        
        # Document extraction
        if name in ("extract_document_content",):
            return ["rule_assist"]
        
        # Multimodal/audio
        if name in ("ask_question_about_image", "ask_question_about_video", "audio_recognition"):
            return ["rule_assist", "self_consistency"]
        
        # Report generation
        if name in ("create_html_report",):
            return ["rule_assist", "self_consistency"]
        
        # Tools not in the list: return no steps
        return []

    def execute(self, messages: List[Dict[str, Any]], step_index=None, max_iteration=10):  # Debug modified to 10
        for i in range(max_iteration):
            logger.info(f'act agent call with tools message: {messages}')
            response = self.llm.create_with_tools(messages, self.tools)
            logger.info(f'act agent call with tools response: {response}')
            
            # DEBUG: Log detailed response information
            logger.debug(f'DEBUG - Response type: {type(response)}')
            logger.debug(f'DEBUG - Response content: {getattr(response, "content", "NO_CONTENT_ATTR")}')
            logger.debug(f'DEBUG - Response tool_calls: {getattr(response, "tool_calls", "NO_TOOL_CALLS_ATTR")}')
            logger.debug(f'DEBUG - Response content is None: {response.content is None}')
            logger.debug(f'DEBUG - Response content == "None": {response.content == "None"}')
            logger.debug(f'DEBUG - Response content == None: {response.content == None}')

            # Process initial response
            result = self._process_response(response, messages, step_index)
            logger.info(f'iter {i} for {self.agent_instance.instance_name} call tools result: {result}')
            logger.debug(f'DEBUG - Result type: {type(result)}')
            logger.debug(f'DEBUG - Result value: {repr(result)}')
            logger.debug(f'DEBUG - Result is truthy: {bool(result)}')
            if result:
                logger.info(f'DEBUG - Returning result: {result}')
                return result

        logger.warning(f'DEBUG - Max iterations ({max_iteration}) reached without meaningful result')
        if max_iteration > 1:
            return self._handle_max_iteration(messages, step_index)
        return messages[-1].get("content")

    def _process_response(self, response, messages, step_index):
        logger.debug(f'DEBUG - _process_response called with response: {response}')
        logger.debug(f'DEBUG - response.tool_calls: {getattr(response, "tool_calls", "NO_TOOL_CALLS")}')
        logger.debug(f'DEBUG - response.content: {repr(getattr(response, "content", "NO_CONTENT"))}')
        
        # Handle the "None" string issue when there are tool calls
        if response.content == "None" and response.tool_calls:
            logger.warning("LLM returned 'None' content with tool calls - this breaks conversation flow")
            # Generate meaningful content based on the tool calls
            tool_names = [call.function.name for call in response.tool_calls]
            response.content = f"I need to use the following tools: {', '.join(tool_names)}"
            logger.info(f"Fixed 'None' content to: {response.content}")
        
        if not response.tool_calls:
            logger.debug(f'DEBUG - No tool calls, returning content: {repr(response.content)}')
            messages.append({"role": "assistant", "content": response.content})
            return response.content

        # Check for early termination based on content
        logger.debug(f'DEBUG - Checking if task is complete for content: {repr(response.content)}')
        if self._is_task_complete(response.content):
            logger.debug(f'DEBUG - Task marked as complete, returning content: {repr(response.content)}')
            messages.append({"role": "assistant", "content": response.content})
            return response.content

        logger.debug(f'DEBUG - Adding assistant message with tool calls')
        messages.append({
            "role": "assistant",
            "content": response.content,
            "tool_calls": response.tool_calls
        })

        logger.debug(f'DEBUG - Executing tool calls: {len(response.tool_calls)} calls')
        results = self._execute_tool_calls(response.tool_calls, step_index)
        logger.debug(f'DEBUG - Tool call results: {len(results)} results')
        messages.extend(results)

        # Check for termination conditions
        logger.debug(f'DEBUG - Checking termination conditions')
        for i, result in enumerate(results):
            logger.debug(f'DEBUG - Result {i}: name={result.get("name", "NO_NAME")}, content={repr(result.get("content", "NO_CONTENT"))}')
            if result["name"] in ["terminate", "mark_step"]:
                logger.debug(f'DEBUG - Found termination tool: {result["name"]}, returning: {repr(result["content"])}')
                return result["content"]
        
        # Return meaningful content instead of None to prevent infinite loops
        if results:
            logger.debug(f'DEBUG - No termination conditions met, returning last tool result: {repr(results[-1]["content"])}')
            return results[-1]["content"]
        else:
            logger.debug(f'DEBUG - No tool results available, returning response content: {repr(response.content)}')
            return response.content

    def _is_task_complete(self, content):
        """Check if the response indicates task completion"""
        if not content:
            return False
        
        completion_indicators = [
            "task completed", "finished", "done", "concluded", "completed successfully",
            "任务完成", "已完成", "结束", "完成", "成功完成",
            "all done", "work finished", "process complete"
        ]
        
        content_lower = content.lower()
        return any(indicator in content_lower for indicator in completion_indicators)

    def _execute_tool_calls(self, tool_calls, step_index):
        if not self._batch_tools:
            return self._execute_tool_calls_original(tool_calls, step_index)
        
        # Batch similar tool calls
        batched_calls = self._batch_similar_calls(tool_calls)
        results = []
        
        with ThreadPoolExecutor() as executor:
            futures = []
            
            for batch in batched_calls:
                if self._can_use_cache(batch):
                    # Use cached results
                    cached_results = self._get_cached_results(batch)
                    results.extend(cached_results)
                else:
                    # Execute batch and cache results
                    futures.append(executor.submit(
                        self._execute_batch_with_caching,
                        batch, step_index
                    ))
            
            # Collect results from futures
            for future in futures:
                try:
                    batch_results = future.result()
                    results.extend(batch_results)
                except Exception as e:
                    logger.error(f"Batch execution error: {e}", exc_info=True)
                    # Fallback to individual execution
                    for tool_call in batch:
                        results.append({
                            "role": "tool",
                            "name": tool_call.function.name,
                            "tool_call_id": tool_call.id,
                            "content": f"Execution error: {str(e)}"
                        })
        
        return results

    def _execute_tool_calls_original(self, tool_calls, step_index):
        """Original tool call execution method"""
        results = []
        with ThreadPoolExecutor() as executor:
            futures = []
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = tool_call.function.arguments

                if function_name in self.functions:
                    futures.append(executor.submit(
                        self._execute_tool_call,
                        function_name=function_name,
                        function_args=function_args,
                        tool_call_id=tool_call.id,
                        step_index=step_index
                    ))
                else:
                    futures.append(executor.submit(
                        self._execute_mcp_tool_call,
                        function_name=function_name,
                        function_args=function_args,
                        tool_call_id=tool_call.id
                    ))

            for future in futures:
                try:
                    results.append(future.result())
                except Exception as e:
                    logger.error(f"Unhandled exception: {e}", exc_info=True)
                    results.append({
                        "role": "tool",
                        "name": function_name,
                        "tool_call_id": tool_call.id,
                        "content": f"Execution error: {str(e)}"
                    })
        return results

    def _batch_similar_calls(self, tool_calls):
        """Group similar tool calls together for batching"""
        batches = []
        current_batch = []
        
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            
            # Check if this call can be batched with current batch
            if (current_batch and 
                current_batch[0].function.name == function_name and
                self._is_cacheable_tool(function_name)):
                current_batch.append(tool_call)
            else:
                if current_batch:
                    batches.append(current_batch)
                current_batch = [tool_call]
        
        if current_batch:
            batches.append(current_batch)
        
        return batches

    def _is_cacheable_tool(self, tool_name):
        """Check if a tool's results can be cached"""
        cacheable_tools = [
            "file_read", "file_find_in_content", "search_google", 
            "search_baidu", "search_wiki", "tavily_search"
        ]
        return tool_name in cacheable_tools

    def _can_use_cache(self, batch):
        """Check if we can use cached results for this batch"""
        if not batch:
            return False
        
        tool_name = batch[0].function.name
        if not self._is_cacheable_tool(tool_name):
            return False
        
        # Check if all calls in batch have cached results
        for tool_call in batch:
            cache_key = self._get_cache_key(tool_call)
            if not self._is_cache_valid(cache_key):
                return False
        
        return True

    def _get_cache_key(self, tool_call):
        """Generate cache key for tool call"""
        function_name = tool_call.function.name
        function_args = tool_call.function.arguments
        
        # Create a hash of the function name and arguments
        key_string = f"{function_name}:{function_args}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def _is_cache_valid(self, cache_key):
        """Check if cache entry is still valid"""
        if cache_key not in self._cache_timestamps:
            return False
        
        current_time = time.time()
        cache_time = self._cache_timestamps[cache_key]
        
        return (current_time - cache_time) < self._cache_ttl

    def _get_cached_results(self, batch):
        """Get cached results for a batch of tool calls"""
        results = []
        for tool_call in batch:
            cache_key = self._get_cache_key(tool_call)
            if cache_key in self._tool_cache:
                cached_result = self._tool_cache[cache_key].copy()
                cached_result["tool_call_id"] = tool_call.id
                results.append(cached_result)
                logger.info(f"Using cached result for {tool_call.function.name}")
        
        return results

    def _execute_batch_with_caching(self, batch, step_index):
        """Execute a batch of tool calls and cache the results"""
        results = []
        
        for tool_call in batch:
            function_name = tool_call.function.name
            function_args = tool_call.function.arguments
            
            # Execute the tool call
            if function_name in self.functions:
                result = self._execute_tool_call(
                    function_name=function_name,
                    function_args=function_args,
                    tool_call_id=tool_call.id,
                    step_index=step_index
                )
            else:
                result = self._execute_mcp_tool_call(
                    function_name=function_name,
                    function_args=function_args,
                    tool_call_id=tool_call.id
                )
            
            results.append(result)
            
            # Cache the result if it's cacheable
            if self._is_cacheable_tool(function_name):
                cache_key = self._get_cache_key(tool_call)
                self._tool_cache[cache_key] = result.copy()
                self._cache_timestamps[cache_key] = time.time()
                logger.info(f"Cached result for {function_name}")
        
        return results

    def _handle_max_iteration(self, messages, step_index):
        messages.append({"role": "user", "content": "Summarize the above conversation, use mark_step to mark the step"})
        mark_step_tools = [tool for tool in self.tools if tool['function']['name'] == 'mark_step']
        response = self.llm.create_with_tools(messages, mark_step_tools)

        result = self._process_response(response, messages, step_index)
        if result:
            return result

        return messages[-1].get("content")

    @time_record
    def _execute_tool_call(self, function_name="", function_args="", tool_call_id="", step_index=None):
        start_time = time.time()
        
        # Push tool start execution event
        self._push_tool_event("tool_start", function_name, function_args, step_index=step_index)
        
        try:
            # Robust JSON arg parsing: tolerate code fences/None/empty/single quotes/trailing commas
            cleaned_args = function_args if function_args is not None else "{}"
            if isinstance(cleaned_args, bytes):
                cleaned_args = cleaned_args.decode('utf-8', errors='ignore')
            cleaned_args = str(cleaned_args).strip()
            # strip markdown fences
            if cleaned_args.startswith("```"):
                tmp = cleaned_args.strip('`')
                if "\n" in tmp:
                    tmp = tmp.split("\n", 1)[1]
                cleaned_args = tmp.strip()
                if cleaned_args.endswith("```"):
                    cleaned_args = cleaned_args[:-3]
            if cleaned_args == "" or cleaned_args.lower() in ("null", "none"):
                cleaned_args = "{}"
            try:
                args_dict = json.loads(cleaned_args)
            except Exception:
                repaired = cleaned_args.replace("'", '"').rstrip(',').strip()
                if repaired and not (repaired.startswith('{') or repaired.startswith('[')):
                    repaired = '{' + repaired + '}'
                try:
                    args_dict = json.loads(repaired)
                except Exception:
                    args_dict = {}

            if step_index is not None and 'step_index' not in args_dict and function_name in ['mark_step']:
                args_dict['step_index'] = step_index

            # Check file_saver call frequency limit
            if function_name == 'file_saver' and step_index is not None:
                if step_index not in self._file_saver_call_count:
                    self._file_saver_call_count[step_index] = 0
                self._file_saver_call_count[step_index] += 1
                
                # If current step has already called file_saver, give warning and suggest consolidation
                if self._file_saver_call_count[step_index] > 1:
                    logger.warning(f"file_saver called {self._file_saver_call_count[step_index]} times in step {step_index}. "
                                 f"Consider consolidating file saves to improve performance.")

            function_to_call = self.functions[function_name]

            # Check if it's an async function
            if inspect.iscoroutinefunction(function_to_call):
                # Create new event loop to run async function
                loop = asyncio.new_event_loop()
                try:
                    asyncio.set_event_loop(loop)
                    # Normalize parameter keys (including function name specific mapping)
                    norm_args = self._normalize_tool_args(function_to_call, args_dict, function_name)
                    result = loop.run_until_complete(function_to_call(**norm_args))
                finally:
                    loop.close()
            else:
                # Call sync function directly
                norm_args = self._normalize_tool_args(function_to_call, args_dict, function_name)
                result = function_to_call(**norm_args)

            # Calculate execution time
            duration = time.time() - start_time
            
            # Push tool execution completion event
            self._push_tool_event("tool_complete", function_name, function_args, 
                                str(result), step_index, duration)

            # Record tool call info to Plan object (if plan reference exists and step_index is valid)
            if self.plan and step_index is not None and hasattr(self.plan, 'add_tool_call'):
                try:
                    self.plan.add_tool_call(step_index, function_name, function_args, str(result))
                except Exception as e:
                    logger.warning(f"Failed to record tool call to plan: {e}")

            return {
                "role": "tool",
                "name": function_name,
                "content": str(result),
                "tool_call_id": tool_call_id
            }
        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            
            # Push tool execution error event
            self._push_tool_event("tool_error", function_name, function_args, 
                                "", step_index, duration, error_msg)
            
            logger.error(f"Unhandled exception: {e}", exc_info=True)
            return {
                "role": "tool",
                "name": function_name,
                "tool_call_id": tool_call_id,
                "content": f"Execution error: {str(e)}"
            }

    @time_record
    def _execute_mcp_tool_call(self, function_name="", function_args="", tool_call_id=""):
        start_time = time.time()
        
        # Push MCP tool start execution event
        self._push_tool_event("tool_start", function_name, function_args, step_index=-1)
        
        loop = None
        try:
            mcp_tool, tool_name = self.find_mcp_tool(function_name)
            if mcp_tool and tool_name:
                cleaned_args = function_args.replace('\\\'', '\'')
                args_dict = json.loads(cleaned_args or "{}")
                # Windows system needs special handling
                if sys.platform == "win32":
                    from asyncio import ProactorEventLoop
                    loop = ProactorEventLoop()
                else:
                    loop = asyncio.new_event_loop()

                asyncio.set_event_loop(loop)

                # Execute async call
                result = loop.run_until_complete(
                    MCPEngine.invoke_mcp_tool(
                        mcp_tool['mcp_name'],
                        mcp_tool['mcp_config'],
                        tool_name,
                        args_dict
                    )
                )
                
                # Calculate execution time
                duration = time.time() - start_time
                
                # Push MCP tool execution completion event
                self._push_tool_event("tool_complete", function_name, function_args, 
                                    str(result), -1, duration)
                
                # Record MCP tool call info to Plan object (if plan reference exists)
                if self.plan and hasattr(self.plan, 'add_tool_call'):
                    try:
                        # MCP tool calls don't have step_index, use -1 to represent global tool calls
                        self.plan.add_tool_call(-1, function_name, function_args, str(result))
                    except Exception as e:
                        logger.warning(f"Failed to record MCP tool call to plan: {e}")
                
                return {
                    "role": "tool",
                    "name": function_name,
                    "content": str(result),
                    "tool_call_id": tool_call_id
                }
            else:
                duration = time.time() - start_time
                error_msg = f"Function {function_name} not found in available functions"
                
                # Push MCP tool execution error event
                self._push_tool_event("tool_error", function_name, function_args, 
                                    "", -1, duration, error_msg)
                
                return {
                    "role": "tool",
                    "name": function_name,
                    "tool_call_id": tool_call_id,
                    "content": error_msg
                }
        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            
            # Push MCP tool execution error event
            self._push_tool_event("tool_error", function_name, function_args, 
                                "", -1, duration, error_msg)
            
            logger.error(f"Unhandled exception: {e}", exc_info=True)
            return {
                "role": "tool",
                "name": function_name,
                "tool_call_id": tool_call_id,
                "content": f"Execution error: {str(e)}"
            }
        finally:
            # Clean up event loop
            if loop:
                loop.close()
                asyncio.set_event_loop(None)
